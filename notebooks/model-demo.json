{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embolism Prediction Model Demo\n",
    "\n",
    "This notebook demonstrates how to use the simplified embolism prediction model with synthetic data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Generate Synthetic Data](#Generate-Synthetic-Data)\n",
    "3. [Load and Prepare Data](#Load-and-Prepare-Data)\n",
    "4. [Train the Model](#Train-the-Model)\n",
    "5. [Evaluate Performance](#Evaluate-Performance)\n",
    "6. [Visualize Results](#Visualize-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "# Add parent directory to path to import modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import model and utilities\n",
    "from models.simplified_embolism_model import EmbolismDataset, EmbolismPredictor, collate_fn\n",
    "from utils.visualization import create_performance_dashboard, plot_training_history\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "If you haven't already generated synthetic data, run the data generator script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "data_path = '../data'\n",
    "cohort_path = os.path.join(data_path, 'cohort.csv')\n",
    "data_file_path = os.path.join(data_path, 'preprocessed_data.csv')\n",
    "\n",
    "if not (os.path.exists(cohort_path) and os.path.exists(data_file_path)):\n",
    "    print(\"Generating synthetic data...\")\n",
    "    # Import data generator\n",
    "    from data.download_demo_data import generate_synthetic_data\n",
    "    generate_synthetic_data(data_path)\n",
    "    print(\"Data generation complete.\")\n",
    "else:\n",
    "    print(\"Using existing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Now let's load the data and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "data = pd.read_csv(data_file_path)\n",
    "cohort = pd.read_csv(cohort_path)\n",
    "\n",
    "# Convert datetime columns\n",
    "datetime_cols = ['charttime', 'chartdate']\n",
    "for col in datetime_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = pd.to_datetime(data[col])\n",
    "\n",
    "# Display data summary\n",
    "print(f\"Loaded data with {len(data)} rows and {len(cohort)} patients\")\n",
    "print(f\"Positive cases (embolism): {cohort['embolism_type'].sum()} patients\")\n",
    "print(f\"Negative cases (no embolism): {len(cohort) - cohort['embolism_type'].sum()} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = EmbolismDataset(\n",
    "    data=data,\n",
    "    cohort=cohort,\n",
    "    time_window=24,\n",
    "    max_patients=200  # Limit dataset size for demonstration\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "print(f\"Splitting data into train ({train_size}), validation ({val_size}), and test ({test_size}) sets\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now let's set up and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'ts_input_dim': 12,       # Number of time series features\n",
    "    'static_input_dim': 9,    # Number of static features\n",
    "    'hidden_dim': 64,         # Hidden dimension\n",
    "    'dropout': 0.2,           # Dropout rate\n",
    "    'learning_rate': 0.001,   # Learning rate\n",
    "    'weight_decay': 1e-5,     # Weight decay\n",
    "    'max_grad_norm': 1.0,     # Gradient clipping\n",
    "    'batch_size': batch_size, # Batch size\n",
    "    'num_epochs': 15,         # Number of epochs\n",
    "    'pos_weight': 5.0,        # Positive class weight\n",
    "    'max_patients': 200       # Maximum number of patients\n",
    "}\n",
    "\n",
    "# Create and train model\n",
    "trainer = EmbolismPredictor(config)\n",
    "history = trainer.train(train_loader, val_loader, config['num_epochs'], patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "Let's evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test AUROC: {test_metrics['auroc']:.4f}\")\n",
    "print(f\"Test AUPRC: {test_metrics['auprc']:.4f}\")\n",
    "print(f\"Test Sensitivity: {test_metrics['sensitivity']:.4f}\")\n",
    "print(f\"Test Specificity: {test_metrics['specificity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Finally, let's visualize the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = trainer.predict(test_loader)\n",
    "\n",
    "# Plot training history\n",
    "history_fig = plot_training_history(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance dashboard\n",
    "dashboard = create_performance_dashboard(\n",
    "    predictions['predictions'],\n",
    "    predictions['labels'],\n",
    "    history=history,\n",
    "    figsize=(16, 12)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Load and prepare synthetic patient data\n",
    "2. Train the embolism prediction model\n",
    "3. Evaluate model performance\n",
    "4. Visualize the results\n",
    "\n",
    "The model shows promising performance even in this simplified implementation. For a real-world application, you would want to use a more comprehensive dataset and potentially the full model architecture described in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
